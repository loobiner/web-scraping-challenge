{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    executable_path = {\"executable_path\": \"C:/users/loobi/OneDrive/Documents/chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "    url = \"https://mars.nasa.gov/news/\"\n",
    "\n",
    "    #scraping up ingredients for soup\n",
    "    r = requests.get(url)\n",
    "\n",
    "    #all html from NASA site\n",
    "    soup = bs(r.content)\n",
    "\n",
    "    #I used the inspect element option on chrome to find that each article on the Mars contains an a element and a div element\n",
    "    #I populated a list called text_elements with all text contained in a or div tags using the whitelist. I then refined that list \n",
    "    #into preferred_text to make the task more manageable. Then I got the title and paragraphs out of the list and assigned them \n",
    "    #to variables\n",
    "\n",
    "    whitelist = ['a', 'div']\n",
    "\n",
    "    text_elements = [t for t in soup.find_all(text=True) if t.parent.name in whitelist]\n",
    "    preferred_text = []\n",
    "    for t in text_elements:\n",
    "        if t != '\\n':\n",
    "            preferred_text.append(t)\n",
    "\n",
    "    news_title = preferred_text[6]\n",
    "    news_p = preferred_text[5]\n",
    "    print(news_title)\n",
    "    print(news_p)\n",
    "    \n",
    "    #image website, using browser to visit site and save html to a list\n",
    "    images_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(images_url)\n",
    "    html = browser.html\n",
    "    images = []\n",
    "    image_soup = bs(html, 'html.parser')\n",
    "\n",
    "    #parsing html containing images and appending images to list\n",
    "    for i in image_soup.find_all('div',class_='img'):\n",
    "        images.append(i.find('img').get('src')) \n",
    "        \n",
    "    #putting together the url to the featured image by splitting and concatenating strings.\n",
    "    split_url = images_url.split('?')\n",
    "    split_images = images[0].split('/')\n",
    "    \n",
    "    featured_image_url = (split_url[0] + split_images[2] + '/' + split_images[3] + '/' + split_images[4])\n",
    "    featured_image_url\n",
    "    \n",
    "    mars_twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "    #ingredients for the soup\n",
    "    browser.visit(mars_twitter_url)\n",
    "\n",
    "    twitter_html = browser.html\n",
    "    # bon apetite\n",
    "    twitter_soup = bs(twitter_html, 'html.parser')\n",
    "\n",
    "    #found that tweets are in a p using inspector, got second tweet because of Mars solar conjunction, second tweet had weather data\n",
    "    p = ['p']\n",
    "    tweets = [tweet for tweet in twitter_soup.find_all(text=True) if tweet.parent.name in p]\n",
    "    mars_weather = tweets[9]\n",
    "    print(mars_weather)\n",
    "    \n",
    "    #url for table\n",
    "    mars_facts_url = 'https://space-facts.com/mars'\n",
    "\n",
    "    #making request\n",
    "    mars_facts = requests.get(mars_facts_url)\n",
    "\n",
    "    #delicious soup\n",
    "    facts_soup = bs(mars_facts.content)\n",
    "\n",
    "    #pulling table from soup\n",
    "    table = facts_soup.find_all('table')\n",
    "\n",
    "    #turning table to html string\n",
    "    facts = pd.read_html(str(table))\n",
    "    \n",
    "    #hemisphere image urls\n",
    "    cerberus_url ='https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced'\n",
    "    schiaparelli_url = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced'\n",
    "    syrtis_major_url = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced'\n",
    "    valles_marineris_url = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced'\n",
    "\n",
    "    hemisphere_images_urls = [\n",
    "                             {'title':'Cerberus Hemisphere', 'image_url':cerberus_url},\n",
    "                             {'title':'Schiaparelli Hemisphere','image_url':schiaparelli_url},\n",
    "                             {'title':'Syrtis Major Hemisphere','image_url':syrtis_major_url},\n",
    "                             {'title':'Valles Marineris Hemisphere','image_url':valles_marineris_url}\n",
    "                             ]\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
